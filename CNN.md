## CNN

### 激励函数

常用的非线性激活函数有Sigmoid、Tanh、Relu等，前两者Sigmoid、Tanh比较常见于全连接层，后者relu常见于卷积层。

#### Sigmod

​	Sigmod的函数表达式为：
$$
g(z) = \frac {1}{1 + e^{-z}}
$$
​	其中z是一个线性组合，比如z可以等于：b + w1 * x1 + w2 * x2。通过代入很大的正数或很小的负数到g(z)函数中可知，其结果趋近于0或1。

#### ReLU

​	Relu的函数表达式为：
$$
ReLU = max (0, x)
$$

### Sigmod 和 ReLU的区别

#### 为什么引入非线性激励函数？

​	如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了。正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络就有意义了（不再是输入的线性组合，可以逼近任意函数）。最早的想法是sigmoid函数或者tanh函数，输出有界，很容易充当下一层输入。

#### 为什么引入ReLU呢？

1. 采用sigmoid等函数，算激活函数时（指数运算），计算量大，反向传播求误差梯度时，求导涉及除法，计算量相对大，而采用Relu激活函数，整个过程的计算量节省很多；
2. 对于深层网络，sigmoid函数反向传播时，很容易就会出现梯度消失的情况（在sigmoid接近饱和区时，变换太缓慢，导数趋于0，这种情况会造成信息丢失），从而无法完成深层网络的训练；
3. Relu会使一部分神经元的输出为0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。

